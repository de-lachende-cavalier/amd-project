- [x] make sure that the data preparation pipeline worked as intended (by inspecting the various pieces)
- [ ] proceed as in Chollet (i.e. bag-of-words => TF-IDF => transformers)
    - [x] bag of words with 1 gram
    - [x] bag of words with 2 gram and TF-IDF
    - [ ] once that's done try different architectures, models, feature engineering (dimensionality reduction)
        - [x] feature engineering
            - [x] check feature importance
            - [x] ? the embedding 'attributes' thing
        - [x] try RNNs (LSTM, GRU)
        - [x] possibly transformers (training time might be excessive)
        - [ ] try transfer learning
        - [ ] maybe try preptrained embeddings ? (word2vec, glove, ...)
- [ ] refactor code
- [ ] make the exposition more fluent (in jupyter notebook)
    - [ ] add images when necessary
    - [ ] possibly split a single notebook into multiple ones?
- [ ] put everything on colab and make sure it works!
    - [ ] not enough RAM!! reduce model size...
- [ ] write the report
